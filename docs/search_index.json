[["index.html", "Bayes Factor Preface", " Bayes Factor Mathematical Statistics Capstone written by Kate Liberko and Isabella Light Preface Hi, thanks for joining us on our journey to learn about Bayes Factor. In this bookdown we will explore Bayes Factor in all of its glory and have fun at the same time! Enjoy :) "],["motivation-of-topic.html", " Chapter 1 Motivation of Topic", " Chapter 1 Motivation of Topic Imagine that you own a botanical collection of houseplants and you just saw a fertilizer commercial. This commercial claims to improve your plants health, making them grow strong and luscious. Your plants have been doing great for years without using any fertilizer, but youre wondering if this new fertilizer is really worth it. This is where Bayes Factor may help you decide.\\(\\\\\\) \\(\\\\\\) A Bayes Factor ratio is used to evaluate data and compare a null hypothesis against an alternative hypothesis (e.g. Null (H0) = fertilizer has no effect on your plants health vs. Alternative (H1) = the new fertilizer has a positive effect on your plants health). In hypothesis testing, the null hypothesis is rejected when the data is weak or can be considered non significant and is accepted otherwise. Knowing your plants are fine without the new fertilizer and also believing your plants could grow strong and lusciously with the new fertilizer, its hard to compare the options or know if you should reject the null hypothesis. However, Bayes Factors are a Bayesian alternative to hypothesis testing. While using Bayes Factors, data is never discarded or rejected, but rather used to create a ratio between the two hypotheses. This ratio provides us with a better understanding of the relationship between the two and allows us to interpret the data in a way that is best for our specific situation. "],["background.html", " Chapter 2 Background 2.1 Bayesian vs Frequentist Statistics 2.2 Hypothesis Testing 2.3 The P-value 2.4 Prior Odds 2.5 Posterior Odds", " Chapter 2 Background In this chapter we will be covering background terminology that is useful for understanding Bayes Factors. This will include Frequentist vs Bayesian statistics and topics within each. These include hypothesis testing and p values under the Frequentist method, and prior and posterior odds under the Bayesian approach. If you already feel comfortable with these topics feel free to skip them. 2.1 Bayesian vs Frequentist Statistics Frequentists believe that function parameters are fixed unknown constants with no distribution. Instead of updating their beliefs, frequentists look at a data set and draw conclusions based on the frequency of this data. Bayesian statistics provide formal methods of incorporating prior knowledge into the estimation of unknown parameters. In Bayesian statistics, parameters are treated as random variables (with a distribution) and not as constants. This allows us to update our beliefs in light of new data with hopes of keeping our models accurate and relevant. Bayesians use a prior distribution multiplied by a likelihood equation to achieve a posterior distribution. For any new data received, this posterior will become the new prior and be multiplied by the likelihood to arrive at an updated posterior. Many statisticians consider Bayesian statistics to be the logical approach to evidence evaluation. 2.2 Hypothesis Testing Hypothesis testing is used to test if the results of an experiment are significant and what the odds are that they happened by chance. One starts by stating two conflicting theories: the null hypothesis \\(H_0\\), and the alternative hypothesis, \\(H_1\\). Data will either support or fail to support your null hypothesis. If your data doesnt support the null, this means that the data probably happened by chance. In this case you will not be able to replicate your experiment and you should reject the null. On the other hand, if your data do support the null hypothesis, this means your data probably didnt happen by chance. This experiment can be replicated and you should fail to reject the null. 2.3 The P-value The p-value in hypothesis testing is the probability of obtaining results equal to or more extreme than the observed results, assuming that the null hypothesis is true. If the p-value is less than  (the significance level), this means that there is a small probability of observing this data if the null hypothesis were true, therefore suggesting that we should reject the null hypothesis. If the p-value is above , then we fail to reject the null. \\(\\\\\\) \\(\\\\\\) The P-Value Approach: \\(\\\\\\) Assume that the null hypothesis is true. \\(\\\\\\) Compute the test statistic \\(z_0 = \\frac{\\bar{X} - \\mu_0}{\\sigma / \\sqrt{n}}\\) \\(\\\\\\) 2.4 Prior Odds A key aspect to Bayesian statistics is the use of prior odds. Priors are an existing belief about the probability of an event. The prior odds are the ratio of the probabilities of the two competing hypotheses. \\[ \\begin{aligned} \\textit{Prior Odds} = \\frac{P(H_1)}{P(H_0)} \\end{aligned} \\] \\(\\\\\\) 2.5 Posterior Odds Posterior odds, are the updated probability. They are the result of taking your prior odds and revising them by taking into account new information. \\[ \\begin{aligned} \\textit{ Posterior Odds}= \\frac{P(H_1| \\textit{ data})}{P(H_0| \\textit{ data})} \\end{aligned} \\] "],["intro-to-bayes-factor.html", " Chapter 3 Intro to Bayes Factor 3.1 A Bayes Walkthrough 3.2 Interpreting The Ratio 3.3 Likelihood Approach to find a Bayes Factor", " Chapter 3 Intro to Bayes Factor When looking at solving a problem by comparing two models, we can take a frequentist approach or a Bayesian approach. If we are trying to decide if its worth it to get this new fertilizer for our plants we will need to gather some data. A frequentist would conduct an experiment and would look at the data and the p-value to make a decision. After collecting data using the new fertilizer, we see that our plants health has increased. The p-value would be the probability of seeing this observed increase in plants health assuming that the null hypothesis (that the fertilizer has no effect) is true. In this case they are only taking into account the results of their experiment. A Bayesian would also conduct an experiment but in addition would consider their prior beliefs. Together the prior beliefs and the data will help them reach a more accurate evaluation of the plants health; the posterior belief. A Bayes Factor, the ratio of the posterior odds to prior odds, will inform them which hypothesis is more likely under the data. Each time that new data is observed, the initial posterior belief will become the new prior belief. Using this, Bayesians can continue to incorporate new data to keep their beliefs and Bayes Factor up to date. For these reasons, we want to use a Bayesian approach for our problem. 3.1 A Bayes Walkthrough Bayes Factor is a Bayesian alternative to classical hypothesis testing. It is used to compare two statistical models (e.g. \\(H_0\\) to \\(H_1\\)) often displayed as a likelihood ratio between the two. It is useful because we are able to update the ratio as new information is gathered which cannot be done under frequentist hypothesis methods. \\(\\\\\\) \\[ \\begin{aligned} \\textit{ Likelihood }H_k &amp; = \\frac{\\textit{ Posterior }H_k}{\\textit{ Prior }H_k}\\\\ \\\\ \\textit{Bayes Factor } &amp; = \\frac{\\textit{ Likelihood }H_1}{\\textit{ Likelihood }H_0}\\\\ \\\\ &amp; = \\frac{\\frac{\\textit{ Posterior }H_1}{\\textit{ Prior }H_1}}{\\frac{\\textit{ Posterior }H_0}{\\textit{ Prior }H_0}}\\\\ \\\\ &amp; = \\frac{\\textit{ Posterior }H_1}{\\textit{ Prior }H_1} \\cdot \\frac{\\textit{ Prior }H_0}{\\textit{ Posterior }H_0}\\\\ \\\\ &amp; = \\frac{\\textit{ Posterior }H_1}{\\textit{ Posterior }H_0} \\cdot \\frac{\\textit{ Prior }H_0}{\\textit{ Prior }H_1}\\\\ \\\\ &amp; = \\frac{P(H_1|\\textit{ data})}{P(H_0|\\textit{ data})} \\cdot \\frac{P(H_0)}{P(H_1)}\\\\ \\\\ &amp; = \\textit{ Posterior Odds} \\cdot \\frac{1}{\\textit{Prior Odds}}\\\\ \\\\ \\textit{Bayes Factor } &amp; = \\frac{\\textit{ Posterior Odds}}{\\textit{Prior Odds}} \\end{aligned} \\] Now that we have some background and some useful equations we can start using this to solve our fertilizer question. We are trying to decide if its worth it to get this new fertilizer for our plants. Lets define our null hypothesis as the event that the fertilizer has no effect on our plants.\\(\\\\\\) \\[ \\begin{aligned} H_0 =\\textit{ Fertilizer has no effect} \\\\ \\end{aligned} \\] Our alternative hypothesis would be that the fertilizer has a good effect on our plants (this is a one-sided alternative hypothesis).\\(\\\\\\) \\[ \\begin{aligned} H_1 =\\textit{Fertilizer has a good effect} \\\\ \\end{aligned} \\] Now lets state our beliefs; these will be our priors for our hypothesis. Maybe we dont have that much faith in the fertilizer, so we believe there is a \\(60\\%\\) chance it doesnt have an effect on our plants and only a \\(40\\%\\) chance it does have an effect on our plants.\\(\\\\\\) \\[ \\begin{aligned}\\textit{ Prior }H_0 = 0.6 \\\\ \\textit{ Prior }H_1 = 0.4 \\\\ \\end{aligned} \\] We start our experiment by measuring our plants health before adding any fertilizer. Since we are a botanist after-all, we can measure our plants photosynthesis process, leaf greenness, and oxygen production. These stats will make up our data which we will call plant stats. These will be used to update our prior beliefs into posterior beliefs after the experiment. After weeks of using the fertilizer, we measure our plant stats again, and using this data, we can now define our posterior beliefs. Almost all of our plants are doing better. Wow, maybe this fertilizer really works! We now believe that the probability of the fertilizer not working is \\(20\\%\\), and the probability of it working is \\(80\\%\\).\\(\\\\\\) \\[ \\begin{aligned}\\textit{Posterior }H_0 = P(H_0 | \\textit{ plant stats}) = 0.2 \\\\ \\textit{Posterior }H_1 = P(H_1 | \\textit{ plant stats}) = 0.8 \\\\ \\end{aligned} \\] Now we can use our data to calculate a Bayes Factor by plugging it into formula (1) from above: \\[ \\begin{aligned} \\textit{Bayes Factor } &amp; = \\frac{P(H_1| \\textit{ plant stats})}{P(H_0|\\textit{ plant stats})} \\cdot \\frac{P(H_0)}{P(H_1)}\\\\ &amp; = \\frac{0.8}{0.2} \\cdot \\frac{0.6}{0.4}\\\\ &amp; = 6 \\end{aligned} \\] A Bayes Factor of 6 means the alternative is 6 times more likely than the null. But what does this mean 3.2 Interpreting The Ratio A great thing about a Bayes Factor is that there is no threshold or value on which to accept or deny anything. Its up to whoever is using the Bayes to decide what the value means to them.\\(\\\\\\) \\(\\\\\\) For example now that you know that the new fertilizer is 6 times more effective than not using fertilizer, would you invest in the fertilizer? That is up to you to decide based on your own situation. Given that you are an expert botanist and your plants are already beautiful, healthy, and thriving, even if this new fertilizer is 6 times more effective, this number may not be significant enough for you to change your routine when you have no problems as is. Lets say your friend however enjoys visiting your house because of all the beautiful plants and has decided to start their own botanical garden. As an amateur botanist, your friend has been having trouble keeping their plants healthy and half of them have died already. They might have a different reaction to the same information that the fertilizer is 6 times more effective. Given that they have not been very successful in growing a botanical garden, the fertilizer could be a game changer for their journey of raising plants. \\(\\\\\\) \\(\\\\\\) In general if the Bayes Factor (set up as \\(\\frac{LikelihoodH_1}{Likelihood H_0}\\)) is greater than one, this means the alternative hypothesis is more likely than the null hypothesis by that factor. If it is less than one, that means the null hypothesis is more likely than the alternative. In the latter we can flip the ratio to get the factor by which the null is more likely than the alternative. In the case that a Bayes Factor is one, this indicates that both models are equally likely. 3.3 Likelihood Approach to find a Bayes Factor In our above example we used simple values for our posterior and priors to get a Bayes Factor. But as you may have noticed the Bayes Factor can also be found using likelihood equations. A Bayes Factor is a weighted average likelihood ratio based on the prior distribution specified for the hypotheses. Since Bayes Factors are an extension of likelihood ratios, lets learn a little about likelihoods and their ratios!\\(\\\\\\) \\(\\\\\\) \\(\\underbrace{\\dfrac{p(H_{0}|E,\\textit{D})}{p(H_{1}|E,\\textit{D})}}_{\\text{Posterior Odds}}= \\underbrace{\\dfrac{p(E|H_{0},\\textit{D})}{p(E|H_{1},\\textit{D})}}_{\\text{Likelihood Ratio}} \\cdot \\underbrace{\\dfrac{p(H_{0}|\\textit{D})}{p(H_{1}|\\textit{D} )}}_{\\text{Prior Odds}}\\) \\(\\\\\\) In this equation we see the likelihood ratio multiplied by the prior odds is equal to the posterior odds. If we divide both sides of the equation by the prior odds we get posterior odds divided by the prior odds are equal to the likelihood ratio. This means the Bayes Factor is the same as the likelihood ratio. \\(\\\\\\) When the hypotheses are simple, say \\(H_0 = 5\\), and \\(H_1 = 6\\), we can find the Bayes Factor by finding the posterior odds and dividing that by the prior odds as done in the plant example above. However, this method will not work if our hypotheses are a range of numbers, say \\(H_0 \\leq 5\\) and \\(H_1 \\geq 5\\). In this case, we must integrate in order to find all possibilities of the hypotheses. We will use equation: \\[ \\begin{aligned} P(D|H_k) = \\int_0^1 P(D| \\theta_k, H_k) \\cdot \\pi(\\theta_k | H_k) d \\theta \\end{aligned} \\] Where \\(P(D|H_k)\\) is the integrated likelihood\\(\\\\\\) \\(P(D| \\theta_k, H_k)\\) is the probability density\\(\\\\\\) \\(\\pi(\\theta_k | H_k)\\) is the prior density (\\(\\pi\\) as a function), and\\(\\\\\\) \\(D\\) as the observed data\\(\\\\\\) \\(\\\\\\) Finding \\(P(D | H_0)\\) and \\(P(D | H_1)\\) by using equation (2), we can find the two likelihood equations and then take the ratio of these to calculate a Bayes Factor. The Bayes Factor is equal to the likelihood ratio. 3.3.1 What is Likelihood? A likelihood equation measures how well a statistical model fits a sample of data for different values of unknown parameters. Likelihood is proportional to a probability; it is not a probability itself. For example, the likelihood of a hypothesis given data is proportional to the probability of that data given the hypothesis is true multiplied by any positive constant: \\(L(H|D) \\propto P(D|H)K\\). A big difference between likelihood and probability is the interpretation of what is what can vary. For conditional probabilities, the hypothesis is fixed while the data varies: \\(P(D|H)\\). Likelihood equations however, predict the likelihood of a hypothesis occurring conditioned on fixed data: \\(L(H|D)\\). \\(\\\\\\) The Law of Likelihood states that within the framework of a statistical model, a particular set of data supports one statistical hypothesis better than another if the likelihood of the first hypothesis, on the data, exceeds the likelihood of the second hypothesis (Etz). The value of a single likelihood is meaningless; only in comparing likelihoods, as we do for a Bayes Factor, do we find meaning. "],["example-problemsexercises.html", " Chapter 4 Example Problems/Exercises 4.1 Using Posteriors and Priors 4.2 Using Likelihoods", " Chapter 4 Example Problems/Exercises 4.1 Using Posteriors and Priors Suppose you are meeting your freshman roommate for the first time. You have a huge love for Star Wars and you find it hard to bond with people that dont share this love. You want to know if you and your roommate for the next year will be close based on their love of Star Wars, but you dont want to be blunt by just asking them. You start with the initial belief that a little more than half the people you come across enjoy Star Wars; there is a \\(60\\%\\) chance that your roommate is a fan and a \\(40\\%\\) chance they are not. \\(\\\\\\) \\[ \\begin{aligned}\\frac{P(\\textit{fan})}{P(\\textit{not fan})}= \\frac{0.6}{0.4} =1.5 \\end{aligned} \\] This means that to start out, they are 1.5 times more likely to be a fan than not. This is your prior belief. You then tell them that you had fun seeing the Star Wars movie that just came out. They tell you that they saw it last weekend. With this, you decide that the chance of having seen the new movie given that a person is a fan is \\(99\\%\\) because almost all fans would be sure to view any new Star Wars movie. But the chance that a person has seen the movie given that they are not a fan is likely around \\(50\\%\\) since nonfans would not make as big an effort to see it but may still be dragged along by friends or family, or simply want to go to the movies as a fun activity. \\(\\\\\\) \\[ \\begin{aligned} \\frac{P(\\textit{seen movie } | \\textit{ fan})}{P(\\textit{seen movie } |\\textit{ not fan})}= \\frac{0.99}{0.5} = 1.98\\\\ \\end{aligned} \\] Knowing that your roommate has seen the movie, you conclude that they are 1.98 times more likely to be a Star Wars fan than not. Putting this together with your prior belief, you calculate your posterior belief: \\(\\\\\\) \\[ \\begin{aligned} \\frac{P(\\textit{fan }| \\textit{ seen movie})}{P(\\textit{not fan }| \\textit{ seen movie} )}= \\frac{P(\\textit{fan})}{P(\\textit{not fan})} \\cdot \\frac{P( \\textit{ seen movie } | \\textit{ fan })}{P(\\textit{seen movie }| \\textit{ not fan })} = \\frac{0.6}{0.4} \\cdot \\frac{0.99}{0.5} = 1.5 \\cdot 1.98 = 2.97 \\end{aligned} \\] You can now see that your roommate is 2.97 times more likely to be a Star Wars fan than not. As you continue your conversation with them and pick up more data such as their dogs name is Anakin, or they own a collection of all the movies back at home, you can continue to update your belief on the chances that they enjoy Star Wars. 4.2 Using Likelihoods Suppose there is a city for which we observe the birth rates of boys and girls to be 49,581 48,870 respectively. We want to know if the probability of male birth is \\(50\\%\\), or if there is an uneven birth rate. Lets define \\(X\\) as the number of male births. Assuming \\(n\\) is the total number of births and \\(\\theta\\) is the probability of a male birth, we can then model this as a binomial distribution \\(X\\sim binom(n, \\theta)\\). We define our null hypothesis to be that male and female births are equally likely or \\(H_0: \\theta = .5\\). Our alternative hypothesis would be that male and female births are not equally likely or \\(H_1: \\theta \\neq .5\\).\\(\\\\\\) \\(\\\\\\) Finding the numerator of our Bayes Factor is pretty straight forward. Since our null hypothesis is a constant (.5) we just need to find the likelihood of the null using the pdf of a binomial distribution. The value \\(n\\) = number of males + number of females = 98451, and \\(k\\) is the number of males. \\[ \\begin{aligned} LikelihoodH_0 &amp; = \\binom{n}{k} \\theta^k (1 - \\theta)^{n -k}\\\\ \\\\ &amp; = \\binom{n}{k} .5^k (1-.5)^{n-k}\\\\ \\\\ &amp; = 1.95 \\cdot 10^{-4} \\end{aligned} \\] The denominator (the alternative) is a bit tougher. Since the alternative could be any range of numbers from 0 to 1 (not including .5), we need to use equation 2 from section 3.3 to integrate over the prior pdf multiplied by the likelihood pdf to find the integrated likelihood. \\(\\\\\\) \\(\\\\\\) \\[ \\begin{aligned} P(X|H_1) = \\int_0^1 P(X| \\theta) f( \\theta) d \\theta \\\\ \\end{aligned} \\] We can assume a Uniform distribution for our prior: \\(Uniform(0,1)\\) prior on theta: \\(f(theta) = 1\\) \\[ \\begin{aligned} P(X|H_1) &amp; = \\int_0^1 P(X| \\theta) d \\theta\\\\ &amp; = \\int_0^1 \\binom{n}{k} \\theta^k (1- \\theta)^{n -k} \\end{aligned} \\] \\(\\binom{n}{k}\\) is a constant, so we can take this out of the integral. \\[ \\begin{aligned} \\binom{n}{k} \\int_0^1 \\theta^k (1- \\theta)^{n -k} = \\binom{n}{k} B(k+1, n-k+ 1) \\end{aligned} \\] This is a Beta distribution. The Beta function has a nice relationship with the gamma function in which it follows the definition of a gamma function when the parameters are positive where: \\(B(x,y) = \\frac{(x-1)! (y-1)!}{(x + y -1)!}\\). Our \\(x = k+1\\) and \\(y = n - k + 1\\) so our equation simplifies to: \\[ \\begin{aligned} \\binom{n}{k} \\frac{k!(n -k)!}{(n+1)!} = 1.0157 \\cdot 10^{-5} \\end{aligned} \\] With this, our Bayes Factor is \\(\\frac{1.02 \\cdot 10-5}{1.95 \\cdot 10-4} =.052\\) with alternative in favor by .05. ( this doesnt make a lot of intuitive sense, so lets flip this ratio)\\(\\\\\\) When we flip it \\(\\frac{1.95\\cdot10-4}{1.02 \\cdot 10-5} = 19.2\\) with null in favor by a factor of 19.2 "],["real-world-applications.html", " Chapter 5 Real World Applications 5.1 Hot Hands 5.2 Ozone Exceedances 5.3 Formation of Mutations in E.Coli DNA", " Chapter 5 Real World Applications Bayes Factors are not just a fun way to make a math textbook longer, they actually have real world applications. Statisticians use Bayes Factors all the time to interpret and compare data models in the real world. We have found some interesting real world applications of Bayes Factors that we explore in this chapter. 5.1 Hot Hands Sports fans often attribute a players good or bad performance in a match to them having a good day or an off day. Hot hands is a term suggesting that the probability of a player making a shot is higher based on their previous performance in that game or the preceding games. It has been suggested that a given players shots may actually follow a binomial distribution, and that these good days and off days may be just random sequences of this larger sample. Misinterpreting these sequences can lead to the assumption that a player has hot hands when in reality this is just a sequence of good shots that happen to be near each other in a larger binomial distribution. In order to test this, data was collected on basketball player Larry Birds game by game performance for an entire season consisting of 44 games. The null hypothesis was that Larry Birds shooting percentages resulted from 44 binomial distributions (one distribution each game) with equal probabilities of success for each shot. That is, 44 different groups of multiple independent flips of a Larry Bird coin. It was found that the null hypothesis fit the data from the season well, however this may have also been due to the sample size being small. In order to check this, an alternative to the binomial could be defined as: the parameters for the binomial distribution vary across each game. The posterior odds of the binomial could then be calculated. This process can be used to evaluate other players and compare their Bayes Factors. 5.2 Ozone Exceedances Rising air-pollution is a topic that is very prevalent in todays world. In the US there are thresholds of ozone levels that must not be exceeded more than 3 times in a given 3-year period. Many cities including Houston, Texas are exceeding this threshold far more than their fair-share. Ozone exceedances can be measured and compared over time to see if Houston and other cities pollution levels are decreasing. These exceedances can be modeled using a Poisson process where the rate is varying over time. Specifically Adrian E Raftery from the American Statistical Association uses three models for rates that represent no change, a gradual decrease, and an abrupt decrease (Kass). They compare these models to see which rate model fits the data best over time. Bayes is not only used for comparison in this case, but also for continuing to update the models based on which models fit the data best. 5.3 Formation of Mutations in E.Coli DNA In a molecular biology experiment, researchers hypothesized that mutations leading to acetate utilization deficiency in the uvrE strain of E. coli bacteria, were caused by an abnormal error-prone DNA repair mechanism and not by DNA replication. If this hypothesis was correct, then the mutation would not be linked to mutations at neighboring loci. But if DNA replication occurred instead of the error-prone DNA repair mechanism, then the mutation would be linked to the rare trait of rifampin resistance. To test the hypothesis, researchers made two rows of cells; one containing selected cells for rifampin resistance and the other made up of unselected cells. The absence of a linkage (predicted by the hypothesis) would suggest that the proportions \\(p_1\\) and \\(p_2\\) of bacteria showing acetate utilization deficiency in both cell lines would be equal. The researchers found that the two proportions were indeed approximately equal and supported their hypothesis of error-prone DNA causing the mutation. However, using this hypothesis test, the researchers were only able accept the null, but they were not able to assess the evidence in favor of the null. Moreover, the researchers had data on 12 other E. coli strains that all showed varying ranges of difference between \\(p_1\\) and \\(p_2\\). Therefore, each strain appeared to have its own level of how it was affected by the error-prone DNA repair mechanism, some being more strongly impacted than others. The fact that they wished to know the strength of the evidence in favor of their hypothesis, along with the fact that they had more data to construct an alternative hypothesis, or prior information, indicate that a Bayesian approach would be most useful. A Bayes Factor would give them a ratio between the null and the alternative hypotheses therefore indicating the degree to which the data favors the null. "],["critiques-of-bayes-factor-approach.html", " Chapter 6 Critiques of Bayes Factor Approach", " Chapter 6 Critiques of Bayes Factor Approach Math is thought to be objective, but the numbers used to calculate Bayes Factors are often subjective, thus raising the question of how trustworthy the resulting ratio actually is. When using Bayesian statistics, you create prior assumptions before any data is gathered, this subjective choice influences the outcome of the analysis. For example, imagine you and a friend are going to an amusement park. This will be your first time ever riding a roller coaster, and you are super nervous. Your friend however, has been coming to this amusement park for years, and assures you that roller coasters are safe, and that you will be fine. The time comes to ride your first coaster and your heart starts beating faster, and you begin to sweat. You believe something will go wrong before the ride is over, but your friend is certain that everything will be fine. Finally, your first ride comes to an end. Youre still in one piece. You and your friend continue to try more and more coasters as you get more comfortable. By the time you leave the park, you realize maybe your fear of roller coasters was all for nothing.\\(\\\\\\) \\(\\\\\\) In this example, you and your friends prior beliefs about roller coasters were drastically different. You believed roller coasters were unsafe and scary, while your friend believed roller coasters are perfectly safe and fun. After many times of testing out and riding roller coasters, your beliefs are being updated. At the end of the day, you and your friends posterior beliefs are the same; you both think roller coasters are safe and fun. This is why even given different prior beliefs using Bayesian methods, posterior beliefs (the end result), still end up the same. This is only the case if given enough time and trials to update the beliefs enough to pull you and your friends beliefs closer together. However, if there are not enough updates or new collections of data, the two differing beliefs may not come together in the end. Say for example, after your first roller coaster ride, a thunder storm arrives shutting down the park for the rest of the day. Since you only had one exposure to a roller coaster ride, you probably still feel the same about roller coasters and you do not believe they are safe nor do you feel comfortable riding. Yet your friend still believes they are not scary at all but in fact super safe. In this case, your final posterior beliefs are different given the same data you collected on the ride. This is an example of when a Bayes factor may be influenced more by opinion and less by observed data. "],["references.html", " Chapter 7 References", " Chapter 7 References Beta Function. Wikipedia, Wikimedia Foundation, 10 Jan. 2021, en.wikipedia.org/wiki/Beta_function.\\(\\\\\\) \\(\\\\\\) Campitelli, Guillermo. Linear Model. Model Comparison: 4. Bayes Factor. YouTube, YouTube, 20 Aug. 2020, www.youtube.com/watch?v=Ke134fEiRDA.\\(\\\\\\) \\(\\\\\\) Clyde, Merlise, et al. An Introduction to Bayesian Thinking. Chapter 3 Losses and Decision Making, statswithr.github.io/book/losses-and-decision-making.html#sec:bayes-factors.\\(\\\\\\) \\(\\\\\\) CrashCourse. You Know Im All About That Bayes: Crash Course Statistics #24. YouTube, YouTube, 25 July 2018, www.youtube.com/watch?v=9TDjifpGj-k&amp;ab_channel=CrashCourse.\\(\\\\\\) \\(\\\\\\) Etz, Alex. Understanding Bayes: A Look at the Likelihood6. The Etz-Files, 12 Sept. 2018, alexanderetz.com/2015/04/15/understanding-bayes-a-look-at-the-likelihood/.\\(\\\\\\) \\(\\\\\\) Halsey Lewis G. 2019. The reign of the p-value is over: what alternative analyses could we employ to fill the power vacuum?Biol. Lett.152019017420190174\\(\\\\\\) \\(\\\\\\) Kass, Robert E, and Adrian E Raftery. Bayes Factors. American Statistical Association, 1995. \\(\\\\\\) \\(\\\\\\) Likelihood function. (2020, November 26). Retrieved from https://en.wikipedia.org/wiki/Likelihood_function$\\\\$ \\(\\\\\\) Lindleys Paradox. Wikipedia, Wikimedia Foundation, 27 Sept. 2020, en.wikipedia.org/wiki/Lindley%27s_paradox#Bayesian_approach. \\(\\\\\\) \\(\\\\\\) Ostrum, R. (2019, September 16). Prior odds - their meaning and significance. Retrieved March 12, 2021, from https://rbostrum.ca/wp/prior-odds-and-their-significance/$\\\\$ \\(\\\\\\) Starmer, Josh. StatQuest: P Values, Clearly Explained. YouTube, YouTube, 20 Nov. 2016, www.youtube.com/watch?v=5Z9OIYA8He8&amp;ab_channel=StatQuestwithJoshStarmer. "]]
